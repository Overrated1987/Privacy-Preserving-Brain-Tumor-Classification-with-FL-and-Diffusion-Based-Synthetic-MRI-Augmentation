{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cropping the images to remove background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def crop_img(img):\n",
    "\t\"\"\"\n",
    "\tFinds the extreme points on the image and crops the rectangular out of them\n",
    "\t\"\"\"\n",
    "\tgray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\tgray = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "\n",
    "\t# threshold the image, then perform a series of erosions +\n",
    "\t# dilations to remove any small regions of noise\n",
    "\tthresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "\tthresh = cv2.erode(thresh, None, iterations=2)\n",
    "\tthresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "\t# find contours in thresholded image, then grab the largest one\n",
    "\tcnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\tcnts = imutils.grab_contours(cnts)\n",
    "\tc = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "\t# find the extreme points\n",
    "\textLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "\textRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "\textTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "\textBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\tADD_PIXELS = 0\n",
    "\tnew_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n",
    "\t\n",
    "\treturn new_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a cropped folder in which cropped training and testing images are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\ttraining = \"archive/Training\"\n",
    "\ttesting = \"archive/Testing\"\n",
    "\ttraining_dir = os.listdir(training)\n",
    "\ttesting_dir = os.listdir(testing)\n",
    "\tIMG_SIZE = 256\n",
    "\t\n",
    "\tfor dir in training_dir:\n",
    "\t\tsave_path = 'Dataset/cropped/Training/'+ dir\n",
    "\t\tpath = os.path.join(training,dir)\n",
    "\t\timage_dir = os.listdir(path)\n",
    "\t\tfor img in image_dir:\n",
    "\t\t\timage = cv2.imread(os.path.join(path,img))\n",
    "\t\t\tnew_img = crop_img(image)\n",
    "\t\t\tnew_img = cv2.resize(new_img,(IMG_SIZE,IMG_SIZE))\n",
    "\t\t\tif not os.path.exists(save_path):\n",
    "\t\t\t\tos.makedirs(save_path)\n",
    "\t\t\tcv2.imwrite(save_path+'/'+img, new_img)\n",
    "\t\n",
    "\tfor dir in testing_dir:\n",
    "\t\tsave_path = 'Dataset/cropped/Testing/'+ dir\n",
    "\t\tpath = os.path.join(testing,dir)\n",
    "\t\timage_dir = os.listdir(path)\n",
    "\t\tfor img in image_dir:\n",
    "\t\t\timage = cv2.imread(os.path.join(path,img))\n",
    "\t\t\tnew_img = crop_img(image)\n",
    "\t\t\tnew_img = cv2.resize(new_img,(IMG_SIZE,IMG_SIZE))\n",
    "\t\t\tif not os.path.exists(save_path):\n",
    "\t\t\t\tos.makedirs(save_path)\n",
    "\t\t\tcv2.imwrite(save_path+'/'+img, new_img)\n",
    "\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glioma', 'meningioma', 'notumor', 'pituitary']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cropped images Demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# img = cv2.imread('./Dataset/Trainingbrain/meningioma/Tr-me_0476.jpg')\n",
    "# img = cv2.resize(\n",
    "#             img,\n",
    "#             dsize=(224,224),\n",
    "#             interpolation=cv2.INTER_CUBIC\n",
    "#         )\n",
    "# gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "# gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "# # threshold the image, then perform a series of erosions +\n",
    "# # dilations to remove any small regions of noise\n",
    "# thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n",
    "# thresh = cv2.erode(thresh, None, iterations=2)\n",
    "# thresh = cv2.dilate(thresh, None, iterations=2)\n",
    "\n",
    "# # find contours in thresholded image, then grab the largest one\n",
    "# cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# cnts = imutils.grab_contours(cnts)\n",
    "# c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "# # find the extreme points\n",
    "# extLeft = tuple(c[c[:, :, 0].argmin()][0])\n",
    "# extRight = tuple(c[c[:, :, 0].argmax()][0])\n",
    "# extTop = tuple(c[c[:, :, 1].argmin()][0])\n",
    "# extBot = tuple(c[c[:, :, 1].argmax()][0])\n",
    "\n",
    "# # add contour on the image\n",
    "# img_cnt = cv2.drawContours(img.copy(), [c], -1, (0, 255, 255), 4)\n",
    "\n",
    "# # add extreme points\n",
    "# img_pnt = cv2.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\n",
    "# img_pnt = cv2.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\n",
    "# img_pnt = cv2.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\n",
    "# img_pnt = cv2.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n",
    "\n",
    "# # crop\n",
    "# ADD_PIXELS = 0\n",
    "# new_img = img[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,6))\n",
    "# plt.subplot(141)\n",
    "# plt.imshow(img)\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# plt.title('Step 1. Get the original image')\n",
    "# plt.subplot(142)\n",
    "# plt.imshow(img_cnt)\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# plt.title('Step 2. Find the biggest contour')\n",
    "# plt.subplot(143)\n",
    "# plt.imshow(img_pnt)\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# plt.title('Step 3. Find the extreme points')\n",
    "# plt.subplot(144)\n",
    "# plt.imshow(new_img)\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# plt.title('Step 4. Crop the image')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating X and Y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5712, 200, 200, 3)\n",
      "(1311, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "image_size = 200\n",
    "labels = ['glioma','meningioma','notumor','pituitary'] \n",
    "for i in labels:\n",
    "    folderpath = os.path.join('./Dataset/Cropped/Training',i)\n",
    "    for j in os.listdir(folderpath):\n",
    "        image = cv2.imread(os.path.join(folderpath,j),0) #load images in gray\n",
    "        image = cv2.bilateralFilter(image,2,50,50) # remove noise image\n",
    "        image = cv2.applyColorMap(image, cv2.COLORMAP_BONE) # produce a pseudocolored image.\n",
    "        image = cv2.resize(image, (image_size, image_size)) # resize images\n",
    "        X_train.append(image)\n",
    "        y_train.append(labels.index(i))\n",
    "        \n",
    "for i in labels:\n",
    "    folderpath = os.path.join('./Dataset/Cropped/Testing',i)\n",
    "    for j in os.listdir(folderpath):\n",
    "        image = cv2.imread(os.path.join(folderpath,j),0) #load images in gray\n",
    "        image = cv2.bilateralFilter(image,2,50,50) # remove noise image\n",
    "        image = cv2.applyColorMap(image, cv2.COLORMAP_BONE) # produce a pseudocolored image.\n",
    "        image = cv2.resize(image, (image_size, image_size)) # resize images\n",
    "        X_test.append(image)\n",
    "        y_test.append(labels.index(i))\n",
    "\n",
    "X_train = np.array(X_train) / 255.0 # normalize Images into range 0 to 1.\n",
    "X_test = np.array(X_test) / 255.0\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5712, 200, 200, 3) signifies the dimensions of the training dataset. \n",
    "\n",
    "- **5712**: This denotes the number of individual samples or images in the training set. Each sample serves as an instance for training the machine learning model.\n",
    "\n",
    "- **200**: This indicates that each image has a width of 200 pixels. Resizing all images to this uniform width ensures compatibility and consistency when feeding them into a model.\n",
    "\n",
    "- **200**: Similarly, this signifies the height of each image, also being 200 pixels. Having a consistent spatial size (width and height) is crucial for most machine learning and deep learning models.\n",
    "\n",
    "- **3**: This represents the three color channels (Red, Green, Blue, or RGB) for each image. Despite initially reading the images in grayscale, the application of `cv2.applyColorMap` seems to have transformed them into color-mapped images with three channels, which is typical for colored images processed by models, especially convolutional neural networks (CNNs) that are designed to handle color information.\n",
    "\n",
    "In summary, `X_train` is a four-dimensional array (or tensor) containing 5712 samples, each being a 200x200 pixel RGB image, well-suited for direct input into deep learning models for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = [X_train[i] for i in range(9)]\n",
    "# fig, axes = plt.subplots(3, 3, figsize = (10, 10))\n",
    "# axes = axes.flatten()\n",
    "# for img, ax in zip(images, axes):\n",
    "#     ax.imshow(img)\n",
    "# #plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train,y_train, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding on the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train) \n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42) #Dividing the dataset into Training and Validation sets.\n",
    "# print(X_val.shape)\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense,Dropout,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator transforms each image in the batch by a series of random translations, rotations, etc.\n",
    "datagen = ImageDataGenerator(\n",
    "     rotation_range=10,                        \n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# After you have created and configured your ImageDataGenerator, you must fit it on your data.\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications.vgg19 import VGG19\n",
    "# IMG_SIZE=(200,200)\n",
    "# base_model = VGG19(\n",
    "#     include_top=False,\n",
    "#     input_shape=IMG_SIZE + (3,),\n",
    "#     weights='imagenet')\n",
    "\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# # Customized layers\n",
    "# x = GlobalAveragePooling2D()(base_model.output)\n",
    "# x = Dropout(0.4)(x)\n",
    "# predict = Dense(4,activation='softmax')(x)\n",
    "\n",
    "# # create a model object\n",
    "# model = Model(inputs = base_model.input,outputs = predict)\n",
    "\n",
    "# #compile our model.\n",
    "# adam = Adam(learning_rate=0.0001)\n",
    "# model.compile(optimizer=adam, loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'test_size' parameter of train_test_split must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got 0.0 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 101\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_X[p], combined_y[p]\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Data Preparation\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Split into client data (80%) and holdout data (20%)\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m X_client, X_holdout, y_client, y_holdout \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHOLDOUT_RATIO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Create non-IID clients from client data portion\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_non_iid_clients\u001b[39m(X, y):\n",
      "File \u001b[1;32mc:\\lzk\\application\\Anaconda\\envs\\FLGPU\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:204\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    202\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 204\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\lzk\\application\\Anaconda\\envs\\FLGPU\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m     )\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'test_size' parameter of train_test_split must be a float in the range (0.0, 1.0), an int in the range [1, inf) or None. Got 0.0 instead."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Federated Learning Parameters\n",
    "NUM_CLASSES = 4  # glioma, meningioma, notumor, pituitary\n",
    "CLIENTS_PER_CLASS = 10  # 5 clients per class\n",
    "NUM_CLIENTS = NUM_CLASSES * CLIENTS_PER_CLASS  # 20 total clients\n",
    "NUM_ROUNDS = 50\n",
    "CLIENT_FRACTION = 0.5  # Fraction of clients participating each round\n",
    "HOLDOUT_RATIO = 0.0  # 10% holdout data\n",
    "HOLDOUT_CLIENT_RATIO = 0.5  # Each client gets 50% of holdout data\n",
    "\n",
    "# Early Stopping Parameters\n",
    "WINDOW_SIZE = 5  # Larger window for non-IID data\n",
    "MIN_DELTA = 0.001  # Smaller delta for non-IID\n",
    "PATIENCE = 5  # More patience for non-IID\n",
    "\n",
    "# Class names for reporting\n",
    "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "def save_classification_report(y_true, y_pred, round_num, save_dir):\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    with open(f\"{save_dir}/classification_report_round_{round_num}.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    return report\n",
    "\n",
    "def save_confusion_matrix(y_true, y_pred, round_num, save_dir):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix (Round {round_num})')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f\"{save_dir}/confusion_matrix_round_{round_num}.pdf\")\n",
    "    plt.close()\n",
    "    return cm\n",
    "\n",
    "def create_global_model():\n",
    "    IMG_SIZE = (200, 200)\n",
    "    base_model = VGG19(\n",
    "        include_top=False,\n",
    "        input_shape=IMG_SIZE + (3,),\n",
    "        weights='imagenet')\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[-7:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.4)(x)\n",
    "    predict = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predict)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                 loss='categorical_crossentropy', \n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_with_holdout(holdout_X, holdout_y):\n",
    "    \"\"\"Train initial model on holdout data\"\"\"\n",
    "    model = create_global_model()\n",
    "    model.fit(\n",
    "        datagen.flow(holdout_X, holdout_y, batch_size=16),\n",
    "        epochs=10,\n",
    "        verbose=1)\n",
    "    return model\n",
    "\n",
    "def create_client_data(X_client, y_client, holdout_X, holdout_y):\n",
    "    \"\"\"Combine client-specific data with shared holdout data\"\"\"\n",
    "    # Get 50% of holdout data (IID)\n",
    "    idx = np.random.choice(len(holdout_X), size=int(len(holdout_X)*HOLDOUT_CLIENT_RATIO), replace=False)\n",
    "    client_holdout_X = holdout_X[idx]\n",
    "    client_holdout_y = holdout_y[idx]\n",
    "    \n",
    "    # Combine with client's non-IID data\n",
    "    combined_X = np.concatenate([X_client, client_holdout_X])\n",
    "    combined_y = np.concatenate([y_client, client_holdout_y])\n",
    "    \n",
    "    # Shuffle the combined data\n",
    "    p = np.random.permutation(len(combined_X))\n",
    "    return combined_X[p], combined_y[p]\n",
    "\n",
    "# Data Preparation\n",
    "# Split into client data (80%) and holdout data (20%)\n",
    "X_client, X_holdout, y_client, y_holdout = train_test_split(\n",
    "    X_train, y_train, test_size=HOLDOUT_RATIO, stratify=np.argmax(y_train, axis=1))\n",
    "\n",
    "# Create non-IID clients from client data portion\n",
    "def create_non_iid_clients(X, y):\n",
    "    y_labels = np.argmax(y, axis=1)\n",
    "    class_data = {}\n",
    "    for class_id in range(NUM_CLASSES):\n",
    "        class_indices = np.where(y_labels == class_id)[0]\n",
    "        class_data[class_id] = X[class_indices], y[class_indices]\n",
    "    \n",
    "    clients = []\n",
    "    for class_id in range(NUM_CLASSES):\n",
    "        X_class, y_class = class_data[class_id]\n",
    "        samples_per_client = len(X_class) // CLIENTS_PER_CLASS\n",
    "        \n",
    "        for client_id in range(CLIENTS_PER_CLASS):\n",
    "            start = client_id * samples_per_client\n",
    "            end = (client_id + 1) * samples_per_client\n",
    "            clients.append((X_class[start:end], y_class[start:end]))\n",
    "    return clients\n",
    "\n",
    "# Create base clients with only non-IID data\n",
    "base_clients = create_non_iid_clients(X_client, y_client)\n",
    "\n",
    "# Create final clients by combining with holdout data\n",
    "clients = [create_client_data(X, y, X_holdout, y_holdout) for X, y in base_clients]\n",
    "\n",
    "# # Data augmentation\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=10,\n",
    "#     width_shift_range=0.05,\n",
    "#     height_shift_range=0.05,\n",
    "#     horizontal_flip=True)\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results_modified', exist_ok=True)\n",
    "os.makedirs('results_modified/reports', exist_ok=True)\n",
    "os.makedirs('results_modified/confusion_matrices', exist_ok=True)\n",
    "\n",
    "# Initialize model on holdout data\n",
    "print(\"Initializing model on holdout data...\")\n",
    "global_model = initialize_with_holdout(X_holdout, y_holdout)\n",
    "\n",
    "# Evaluate and save initialization model performance (Round 0)\n",
    "print(\"\\nEvaluating initialization model (Round 0)...\")\n",
    "y_pred = global_model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Save initialization model reports\n",
    "save_classification_report(y_true, y_pred_classes, 0, 'results_modified/reports')\n",
    "save_confusion_matrix(y_true, y_pred_classes, 0, 'results_modified/confusion_matrices')\n",
    "\n",
    "initial_loss, initial_accuracy = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Initial model accuracy: {initial_accuracy:.4f}, Loss: {initial_loss:.4f}\")\n",
    "\n",
    "# Initialize tracking variables with initial model performance\n",
    "results = [{\n",
    "    'round': 0,\n",
    "    'accuracy': initial_accuracy,\n",
    "    'loss': initial_loss\n",
    "}]\n",
    "\n",
    "best_accuracy = initial_accuracy\n",
    "best_model_weights = global_model.get_weights()\n",
    "recent_accuracies = [initial_accuracy]\n",
    "no_improvement_count = 0\n",
    "early_stop_triggered = False \n",
    "\n",
    "# Federated Learning Main Loop\n",
    "\n",
    "for round_num in range(NUM_ROUNDS):\n",
    "    if early_stop_triggered:\n",
    "        print(\"Early stopping triggered. Ending training.\")\n",
    "        break\n",
    "        \n",
    "    print(f\"\\n=== Federated Learning Round {round_num + 1}/{NUM_ROUNDS} ===\")\n",
    "    \n",
    "    # Select clients\n",
    "    selected_indices = []\n",
    "    for class_id in range(NUM_CLASSES):\n",
    "        class_client_indices = range(class_id*CLIENTS_PER_CLASS, (class_id+1)*CLIENTS_PER_CLASS)\n",
    "        selected = np.random.choice(\n",
    "            list(class_client_indices),\n",
    "            size=max(1, int(CLIENT_FRACTION * CLIENTS_PER_CLASS)),\n",
    "            replace=False)\n",
    "        selected_indices.extend(selected)\n",
    "    \n",
    "    selected_clients = [clients[i] for i in selected_indices]\n",
    "    \n",
    "    # Local training\n",
    "    client_weights = []\n",
    "    client_sizes = []\n",
    "    \n",
    "    for client_idx, (client_X, client_y) in enumerate(selected_clients):\n",
    "        print(f\"\\nTraining Client {client_idx + 1}/{len(selected_clients)}...\")\n",
    "        \n",
    "        client_model = tf.keras.models.clone_model(global_model)\n",
    "        client_model.set_weights(global_model.get_weights())\n",
    "        client_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "        history = client_model.fit(\n",
    "            datagen.flow(client_X, client_y, batch_size=16),\n",
    "            epochs=5,\n",
    "            verbose=1)\n",
    "        \n",
    "        client_weights.append(client_model.get_weights())\n",
    "        client_sizes.append(len(client_X))\n",
    "    \n",
    "    # Federated Averaging\n",
    "    global_weights = []\n",
    "    total_size = sum(client_sizes)\n",
    "    \n",
    "    for layer_idx in range(len(global_model.get_weights())):\n",
    "        weighted_sum = np.zeros_like(global_model.get_weights()[layer_idx])\n",
    "        for client_idx in range(len(client_weights)):\n",
    "            weighted_sum += client_weights[client_idx][layer_idx] * (client_sizes[client_idx] / total_size)\n",
    "        global_weights.append(weighted_sum)\n",
    "    \n",
    "    global_model.set_weights(global_weights)\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred = global_model.predict(X_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    save_classification_report(y_true, y_pred_classes, round_num+1, 'results_modified/reports')\n",
    "    save_confusion_matrix(y_true, y_pred_classes, round_num+1, 'results_modified/confusion_matrices')\n",
    "    \n",
    "    loss, accuracy = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    results.append({\n",
    "        'round': round_num + 1,\n",
    "        'accuracy': accuracy,\n",
    "        'loss': loss\n",
    "    })\n",
    "    \n",
    "    if accuracy > best_accuracy + MIN_DELTA:\n",
    "        best_accuracy = accuracy\n",
    "        best_model_weights = global_model.get_weights()\n",
    "        no_improvement_count = 0\n",
    "        print(f\"New best accuracy: {best_accuracy:.4f}\")\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    recent_accuracies.append(accuracy)\n",
    "    if len(recent_accuracies) >= WINDOW_SIZE:\n",
    "        window_avg = np.mean(recent_accuracies[-WINDOW_SIZE:])\n",
    "        prev_window_avg = np.mean(recent_accuracies[-WINDOW_SIZE-1:-1] if len(recent_accuracies) > WINDOW_SIZE else recent_accuracies[:-1])\n",
    "        \n",
    "        if window_avg <= prev_window_avg + MIN_DELTA:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"No improvement count: {no_improvement_count}/{PATIENCE}\")\n",
    "        else:\n",
    "            no_improvement_count = max(0, no_improvement_count - 1)\n",
    "        \n",
    "        if no_improvement_count >= PATIENCE:\n",
    "            early_stop_triggered = True\n",
    "\n",
    "# Save results and models\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('results_modified/training_results.csv', index=False)\n",
    "\n",
    "global_model.set_weights(best_model_weights)\n",
    "global_model.save('results_modified/best_global_model.h5')\n",
    "\n",
    "# Final evaluation\n",
    "y_pred = global_model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "save_classification_report(y_true, y_pred_classes, 'final', 'results_modified/reports')\n",
    "save_confusion_matrix(y_true, y_pred_classes, 'final', 'results_modified/confusion_matrices')\n",
    "\n",
    "global_model.save('results_modified/final_global_model.h5')\n",
    "print(\"\\nFederated learning completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.applications.vgg19 import VGG19\n",
    "# from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "# from tensorflow.keras.models import Model, clone_model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# # ======================== 全局配置 ========================\n",
    "# # 数据集参数\n",
    "# NUM_CLASSES = 4  # glioma, meningioma, notumor, pituitary\n",
    "# class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "# # 联邦学习参数\n",
    "# CLIENTS_PER_CLASS = 5  # 每类客户端数量\n",
    "# NUM_CLIENTS = NUM_CLASSES * CLIENTS_PER_CLASS  # 总客户端数\n",
    "# NUM_ROUNDS = 50  # 联邦学习总轮次\n",
    "# CLIENT_FRACTION = 0.5  # 每轮选择的客户端比例\n",
    "# HOLDOUT_RATIO = 0.15  # 保留数据比例\n",
    "# HOLDOUT_CLIENT_RATIO = 0.5  # 每个客户端获得的保留数据比例\n",
    "\n",
    "# # 训练参数\n",
    "# BATCH_SIZE = 16\n",
    "# CLIENT_EPOCHS = 5\n",
    "# INITIAL_EPOCHS = 10\n",
    "# LEARNING_RATE = 0.0001\n",
    "\n",
    "# # 早停参数\n",
    "# WINDOW_SIZE = 5\n",
    "# MIN_DELTA = 0.001\n",
    "# PATIENCE = 5\n",
    "\n",
    "# # ======================== 数据准备函数 ========================\n",
    "# def prepare_data(X_train: np.ndarray, \n",
    "#                 y_train: np.ndarray,\n",
    "#                 test_size: float = HOLDOUT_RATIO) -> Tuple:\n",
    "#     \"\"\"\n",
    "#     准备联邦学习数据（客户端数据 + 保留数据）\n",
    "#     Returns:\n",
    "#         clients: 客户端数据列表 [(X1, y1), ...]\n",
    "#         X_holdout: 保留特征数据\n",
    "#         y_holdout: 保留标签数据\n",
    "#     \"\"\"\n",
    "#     # 分割客户端数据和保留数据\n",
    "#     X_client, X_holdout, y_client, y_holdout = train_test_split(\n",
    "#         X_train, y_train, test_size=test_size, stratify=np.argmax(y_train, axis=1))\n",
    "    \n",
    "#     # 创建非IID客户端\n",
    "#     base_clients = _create_non_iid_clients(X_client, y_client)\n",
    "    \n",
    "#     # 为每个客户端添加部分保留数据\n",
    "#     clients = [_add_holdout_data(client, X_holdout, y_holdout) for client in base_clients]\n",
    "    \n",
    "#     return clients, X_holdout, y_holdout\n",
    "\n",
    "# def _create_non_iid_clients(X: np.ndarray, \n",
    "#                           y: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "#     \"\"\"创建非IID分布的客户端数据\"\"\"\n",
    "#     y_labels = np.argmax(y, axis=1)\n",
    "#     class_data = {}\n",
    "#     for class_id in range(NUM_CLASSES):\n",
    "#         class_indices = np.where(y_labels == class_id)[0]\n",
    "#         class_data[class_id] = X[class_indices], y[class_indices]\n",
    "    \n",
    "#     clients = []\n",
    "#     for class_id in range(NUM_CLASSES):\n",
    "#         X_class, y_class = class_data[class_id]\n",
    "#         samples_per_client = len(X_class) // CLIENTS_PER_CLASS\n",
    "        \n",
    "#         for client_id in range(CLIENTS_PER_CLASS):\n",
    "#             start = client_id * samples_per_client\n",
    "#             end = (client_id + 1) * samples_per_client\n",
    "#             clients.append((X_class[start:end], y_class[start:end]))\n",
    "#     return clients\n",
    "\n",
    "# def _add_holdout_data(client_data: Tuple[np.ndarray, np.ndarray],\n",
    "#                      X_holdout: np.ndarray,\n",
    "#                      y_holdout: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "#     \"\"\"为客户端添加部分保留数据\"\"\"\n",
    "#     X_client, y_client = client_data\n",
    "    \n",
    "#     # 随机选择部分保留数据\n",
    "#     idx = np.random.choice(len(X_holdout), \n",
    "#                           size=int(len(X_holdout)*HOLDOUT_CLIENT_RATIO), \n",
    "#                           replace=False)\n",
    "#     client_holdout_X = X_holdout[idx]\n",
    "#     client_holdout_y = y_holdout[idx]\n",
    "    \n",
    "#     # 合并并打乱\n",
    "#     combined_X = np.concatenate([X_client, client_holdout_X])\n",
    "#     combined_y = np.concatenate([y_client, client_holdout_y])\n",
    "#     p = np.random.permutation(len(combined_X))\n",
    "    \n",
    "#     return combined_X[p], combined_y[p]\n",
    "\n",
    "# # ======================== 模型相关函数 ========================\n",
    "# def create_model(input_shape: Tuple[int, int, int] = (200, 200, 3),\n",
    "#                 base_trainable_layers: int = 7) -> tf.keras.Model:\n",
    "#     \"\"\"创建VGG19基础模型\"\"\"\n",
    "#     base_model = VGG19(\n",
    "#         include_top=False,\n",
    "#         input_shape=input_shape,\n",
    "#         weights='imagenet')\n",
    "\n",
    "#     # 冻结大部分层\n",
    "#     for layer in base_model.layers:\n",
    "#         layer.trainable = False\n",
    "#     # 解冻最后几层\n",
    "#     for layer in base_model.layers[-base_trainable_layers:]:\n",
    "#         layer.trainable = True\n",
    "\n",
    "#     # 添加自定义层\n",
    "#     x = GlobalAveragePooling2D()(base_model.output)\n",
    "#     x = Dropout(0.4)(x)\n",
    "#     predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "#     model = Model(inputs=base_model.input, outputs=predictions)\n",
    "#     model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "#                  loss='categorical_crossentropy',\n",
    "#                  metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def initialize_global_model(X_holdout: np.ndarray,\n",
    "#                           y_holdout: np.ndarray) -> tf.keras.Model:\n",
    "#     \"\"\"在保留数据上初始化全局模型\"\"\"\n",
    "#     model = create_model()\n",
    "#     model.fit(\n",
    "#         datagen.flow(X_holdout, y_holdout, batch_size=BATCH_SIZE),\n",
    "#         epochs=INITIAL_EPOCHS,\n",
    "#         verbose=1)\n",
    "#     return model\n",
    "\n",
    "# # ======================== 联邦学习核心函数 ========================\n",
    "# def select_clients(clients: List[Tuple[np.ndarray, np.ndarray]], \n",
    "#                   client_fraction: float = CLIENT_FRACTION) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "#     \"\"\"分层抽样选择客户端\"\"\"\n",
    "#     selected_indices = []\n",
    "#     for class_id in range(NUM_CLASSES):\n",
    "#         class_client_indices = range(class_id*CLIENTS_PER_CLASS, (class_id+1)*CLIENTS_PER_CLASS)\n",
    "#         selected = np.random.choice(\n",
    "#             list(class_client_indices),\n",
    "#             size=max(1, int(client_fraction * CLIENTS_PER_CLASS)),\n",
    "#             replace=False)\n",
    "#         selected_indices.extend(selected)\n",
    "#     return [clients[i] for i in selected_indices]\n",
    "\n",
    "# def client_train(global_model: tf.keras.Model,\n",
    "#                 client_data: Tuple[np.ndarray, np.ndarray],\n",
    "#                 epochs: int = CLIENT_EPOCHS) -> Tuple[List[np.ndarray], int]:\n",
    "#     \"\"\"客户端本地训练\"\"\"\n",
    "#     # 克隆全局模型\n",
    "#     client_model = clone_model(global_model)\n",
    "#     client_model.set_weights(global_model.get_weights())\n",
    "    \n",
    "#     # 编译模型\n",
    "#     client_model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "#                        loss='categorical_crossentropy',\n",
    "#                        metrics=['accuracy'])\n",
    "    \n",
    "#     # 训练\n",
    "#     client_model.fit(\n",
    "#         datagen.flow(client_data[0], client_data[1], batch_size=BATCH_SIZE),\n",
    "#         epochs=epochs,\n",
    "#         verbose=1)\n",
    "    \n",
    "#     return client_model.get_weights(), len(client_data[0])\n",
    "\n",
    "# def federated_average(global_model: tf.keras.Model,\n",
    "#                      client_weights: List[List[np.ndarray]],\n",
    "#                      client_sizes: List[int]) -> List[np.ndarray]:\n",
    "#     \"\"\"联邦平均聚合\"\"\"\n",
    "#     global_weights = []\n",
    "#     total_size = sum(client_sizes)\n",
    "    \n",
    "#     for layer_idx in range(len(global_model.get_weights())):\n",
    "#         weighted_sum = np.zeros_like(global_model.get_weights()[layer_idx])\n",
    "#         for client_idx in range(len(client_weights)):\n",
    "#             weighted_sum += client_weights[client_idx][layer_idx] * (client_sizes[client_idx] / total_size)\n",
    "#         global_weights.append(weighted_sum)\n",
    "    \n",
    "#     return global_weights\n",
    "\n",
    "# # ======================== 评估与工具函数 ========================\n",
    "# def evaluate_model(model: tf.keras.Model,\n",
    "#                   X_test: np.ndarray,\n",
    "#                   y_test: np.ndarray,\n",
    "#                   round_num: int,\n",
    "#                   save_dir: str) -> Tuple[float, float]:\n",
    "#     \"\"\"模型评估与结果保存\"\"\"\n",
    "#     # 预测\n",
    "#     y_pred = model.predict(X_test, verbose=0)\n",
    "#     y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "#     y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "#     # 保存报告\n",
    "#     _save_classification_report(y_true, y_pred_classes, round_num, save_dir)\n",
    "#     _save_confusion_matrix(y_true, y_pred_classes, round_num, save_dir)\n",
    "    \n",
    "#     # 计算指标\n",
    "#     loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "#     print(f\"Round {round_num} - Test Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\")\n",
    "    \n",
    "#     return loss, accuracy\n",
    "\n",
    "# def _save_classification_report(y_true: np.ndarray,\n",
    "#                               y_pred: np.ndarray,\n",
    "#                               round_num: int,\n",
    "#                               save_dir: str):\n",
    "#     \"\"\"保存分类报告\"\"\"\n",
    "#     report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "#     os.makedirs(f\"{save_dir}/reports\", exist_ok=True)\n",
    "#     with open(f\"{save_dir}/reports/classification_report_round_{round_num}.txt\", \"w\") as f:\n",
    "#         f.write(report)\n",
    "\n",
    "# def _save_confusion_matrix(y_true: np.ndarray,\n",
    "#                          y_pred: np.ndarray,\n",
    "#                          round_num: int,\n",
    "#                          save_dir: str):\n",
    "#     \"\"\"保存混淆矩阵\"\"\"\n",
    "#     cm = confusion_matrix(y_true, y_pred)\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "#                 xticklabels=class_names, yticklabels=class_names)\n",
    "#     plt.title(f'Confusion Matrix (Round {round_num})')\n",
    "#     plt.ylabel('True Label')\n",
    "#     plt.xlabel('Predicted Label')\n",
    "    \n",
    "#     os.makedirs(f\"{save_dir}/confusion_matrices\", exist_ok=True)\n",
    "#     plt.savefig(f\"{save_dir}/confusion_matrices/round_{round_num}.pdf\")\n",
    "#     plt.close()\n",
    "\n",
    "# def check_early_stopping(recent_accuracies: List[float],\n",
    "#                         best_accuracy: float,\n",
    "#                         no_improvement_count: int,\n",
    "#                         window_size: int = WINDOW_SIZE,\n",
    "#                         min_delta: float = MIN_DELTA,\n",
    "#                         patience: int = PATIENCE) -> Tuple[bool, float, int]:\n",
    "#     \"\"\"早停检查\"\"\"\n",
    "#     current_accuracy = recent_accuracies[-1]\n",
    "    \n",
    "#     # 更新最佳准确率\n",
    "#     if current_accuracy > best_accuracy + min_delta:\n",
    "#         best_accuracy = current_accuracy\n",
    "#         no_improvement_count = 0\n",
    "#         print(f\"New best accuracy: {best_accuracy:.4f}\")\n",
    "#     else:\n",
    "#         no_improvement_count += 1\n",
    "    \n",
    "#     # 滑动窗口检查\n",
    "#     if len(recent_accuracies) >= window_size:\n",
    "#         window_avg = np.mean(recent_accuracies[-window_size:])\n",
    "#         prev_window_avg = np.mean(recent_accuracies[-window_size-1:-1] \n",
    "#                                 if len(recent_accuracies) > window_size \n",
    "#                                 else recent_accuracies[:-1])\n",
    "        \n",
    "#         if window_avg <= prev_window_avg + min_delta:\n",
    "#             no_improvement_count += 1\n",
    "#             print(f\"No improvement count: {no_improvement_count}/{patience}\")\n",
    "    \n",
    "#     should_stop = no_improvement_count >= patience\n",
    "#     return should_stop, best_accuracy, no_improvement_count\n",
    "\n",
    "# # ======================== 主流程 ========================\n",
    "# def run_federated_learning(X_train: np.ndarray,\n",
    "#                          y_train: np.ndarray,\n",
    "#                          X_test: np.ndarray,\n",
    "#                          y_test: np.ndarray,\n",
    "#                          output_dir: str = \"results\") -> Dict[str, Any]:\n",
    "#     \"\"\"执行完整的联邦学习流程\"\"\"\n",
    "#     # 准备目录\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # 准备数据\n",
    "#     clients, X_holdout, y_holdout = prepare_data(X_train, y_train)\n",
    "    \n",
    "#     # 初始化全局模型\n",
    "#     global_model = initialize_global_model(X_holdout, y_holdout)\n",
    "    \n",
    "#     # 初始评估\n",
    "#     initial_loss, initial_accuracy = evaluate_model(\n",
    "#         global_model, X_test, y_test, 0, output_dir)\n",
    "    \n",
    "#     # 初始化训练状态\n",
    "#     training_state = {\n",
    "#         'results': [{'round': 0, 'accuracy': initial_accuracy, 'loss': initial_loss}],\n",
    "#         'best_accuracy': initial_accuracy,\n",
    "#         'best_model_weights': global_model.get_weights(),\n",
    "#         'recent_accuracies': [initial_accuracy],\n",
    "#         'no_improvement_count': 0,\n",
    "#         'early_stop_triggered': False\n",
    "#     }\n",
    "\n",
    "#     # 联邦学习主循环\n",
    "#     for round_num in range(1, NUM_ROUNDS + 1):\n",
    "#         if training_state['early_stop_triggered']:\n",
    "#             print(\"Early stopping triggered. Ending training.\")\n",
    "#             break\n",
    "            \n",
    "#         print(f\"\\n=== Federated Learning Round {round_num}/{NUM_ROUNDS} ===\")\n",
    "        \n",
    "#         # 1. 客户端选择\n",
    "#         selected_clients = select_clients(clients)\n",
    "        \n",
    "#         # 2. 本地训练\n",
    "#         client_weights, client_sizes = [], []\n",
    "#         for client_data in selected_clients:\n",
    "#             weights, size = client_train(global_model, client_data)\n",
    "#             client_weights.append(weights)\n",
    "#             client_sizes.append(size)\n",
    "        \n",
    "#         # 3. 模型聚合\n",
    "#         global_weights = federated_average(global_model, client_weights, client_sizes)\n",
    "#         global_model.set_weights(global_weights)\n",
    "        \n",
    "#         # 4. 评估\n",
    "#         loss, accuracy = evaluate_model(\n",
    "#             global_model, X_test, y_test, round_num, output_dir)\n",
    "        \n",
    "#         # 更新训练状态\n",
    "#         training_state['results'].append({\n",
    "#             'round': round_num,\n",
    "#             'accuracy': accuracy,\n",
    "#             'loss': loss\n",
    "#         })\n",
    "#         training_state['recent_accuracies'].append(accuracy)\n",
    "        \n",
    "#         # 5. 早停检查\n",
    "#         should_stop, new_best, new_count = check_early_stopping(\n",
    "#             training_state['recent_accuracies'],\n",
    "#             training_state['best_accuracy'],\n",
    "#             training_state['no_improvement_count'])\n",
    "            \n",
    "#         training_state.update({\n",
    "#             'best_accuracy': new_best,\n",
    "#             'no_improvement_count': new_count,\n",
    "#             'early_stop_triggered': should_stop\n",
    "#         })\n",
    "        \n",
    "#         # 保存最佳模型\n",
    "#         if new_best == accuracy:\n",
    "#             training_state['best_model_weights'] = global_model.get_weights()\n",
    "\n",
    "#     # 保存最终结果\n",
    "#     _save_final_results(global_model, training_state, output_dir)\n",
    "    \n",
    "#     return training_state\n",
    "\n",
    "# def _save_final_results(model: tf.keras.Model,\n",
    "#                       training_state: Dict[str, Any],\n",
    "#                       output_dir: str):\n",
    "#     \"\"\"保存最终结果和模型\"\"\"\n",
    "#     # 保存训练结果\n",
    "#     pd.DataFrame(training_state['results']).to_csv(\n",
    "#         f\"{output_dir}/training_results.csv\", index=False)\n",
    "    \n",
    "#     # 保存最佳模型\n",
    "#     model.set_weights(training_state['best_model_weights'])\n",
    "#     model.save(f\"{output_dir}/best_model.h5\")\n",
    "    \n",
    "#     # 最终评估\n",
    "#     y_pred = model.predict(X_test, verbose=0)\n",
    "#     y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "#     y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "#     _save_classification_report(y_true, y_pred_classes, 'final', output_dir)\n",
    "#     _save_confusion_matrix(y_true, y_pred_classes, 'final', output_dir)\n",
    "    \n",
    "#     model.save(f\"{output_dir}/final_model.h5\")\n",
    "\n",
    "# # ======================== 数据增强 ========================\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=10,\n",
    "#     width_shift_range=0.05,\n",
    "#     height_shift_range=0.05,\n",
    "#     horizontal_flip=True)\n",
    "\n",
    "# # ======================== 执行入口 ========================\n",
    "# if __name__ == \"__main__\":\n",
    "#     # X_train, y_train, X_test, y_test = load_your_dataset()\n",
    "    \n",
    "#     # 执行联邦学习\n",
    "#     results = run_federated_learning(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "#     print(\"\\nTraining completed. Results saved to 'results' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.models import Model, clone_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 数据准备函数 ========================\n",
    "def prepare_data(X_train: np.ndarray, \n",
    "                y_train: np.ndarray,\n",
    "                test_size: float = HOLDOUT_RATIO) -> Tuple:\n",
    "    \"\"\"\n",
    "    准备联邦学习数据（客户端数据 + 保留数据）\n",
    "    Returns:\n",
    "        clients: 客户端数据列表 [(X1, y1), ...]\n",
    "        X_holdout: 保留特征数据\n",
    "        y_holdout: 保留标签数据\n",
    "    \"\"\"\n",
    "    # 分割客户端数据和保留数据\n",
    "    X_client, X_holdout, y_client, y_holdout = train_test_split(\n",
    "        X_train, y_train, test_size=test_size, stratify=np.argmax(y_train, axis=1))\n",
    "    \n",
    "    # 创建非IID客户端\n",
    "    base_clients = _create_non_iid_clients(X_client, y_client)\n",
    "    \n",
    "    # 为每个客户端添加部分保留数据\n",
    "    clients = [_add_holdout_data(client, X_holdout, y_holdout) for client in base_clients]\n",
    "    \n",
    "    return clients, X_holdout, y_holdout\n",
    "\n",
    "def _create_non_iid_clients(X: np.ndarray, \n",
    "                          y: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"创建非IID分布的客户端数据\"\"\"\n",
    "    y_labels = np.argmax(y, axis=1)\n",
    "    class_data = {}\n",
    "    for class_id in range(NUM_CLASSES):\n",
    "        class_indices = np.where(y_labels == class_id)[0]\n",
    "        class_data[class_id] = X[class_indices], y[class_indices]\n",
    "    \n",
    "    clients = []\n",
    "    for class_id in range(NUM_CLASSES):\n",
    "        X_class, y_class = class_data[class_id]\n",
    "        samples_per_client = len(X_class) // CLIENTS_PER_CLASS\n",
    "        \n",
    "        for client_id in range(CLIENTS_PER_CLASS):\n",
    "            start = client_id * samples_per_client\n",
    "            end = (client_id + 1) * samples_per_client\n",
    "            clients.append((X_class[start:end], y_class[start:end]))\n",
    "    return clients\n",
    "\n",
    "def _add_holdout_data(client_data: Tuple[np.ndarray, np.ndarray],\n",
    "                     X_holdout: np.ndarray,\n",
    "                     y_holdout: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"为客户端添加部分保留数据\"\"\"\n",
    "    X_client, y_client = client_data\n",
    "    \n",
    "    # 随机选择部分保留数据\n",
    "    idx = np.random.choice(len(X_holdout), \n",
    "                          size=int(len(X_holdout)*HOLDOUT_CLIENT_RATIO), \n",
    "                          replace=False)\n",
    "    client_holdout_X = X_holdout[idx]\n",
    "    client_holdout_y = y_holdout[idx]\n",
    "    \n",
    "    # 合并并打乱\n",
    "    combined_X = np.concatenate([X_client, client_holdout_X])\n",
    "    combined_y = np.concatenate([y_client, client_holdout_y])\n",
    "    p = np.random.permutation(len(combined_X))\n",
    "    \n",
    "    return combined_X[p], combined_y[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 模型相关函数 ========================\n",
    "def create_model(input_shape: Tuple[int, int, int] = (200, 200, 3),\n",
    "                base_trainable_layers: int = 7) -> tf.keras.Model:\n",
    "    \"\"\"创建VGG19基础模型\"\"\"\n",
    "    base_model = VGG19(\n",
    "        include_top=False,\n",
    "        input_shape=input_shape,\n",
    "        weights='imagenet')\n",
    "\n",
    "    # 冻结大部分层\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    # 解冻最后几层\n",
    "    for layer in base_model.layers[-base_trainable_layers:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # 添加自定义层\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.4)(x)\n",
    "    predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def initialize_global_model(X_holdout: np.ndarray,\n",
    "                          y_holdout: np.ndarray) -> tf.keras.Model:\n",
    "    \"\"\"在保留数据上初始化全局模型\"\"\"\n",
    "    model = create_model()\n",
    "    model.fit(\n",
    "        datagen.flow(X_holdout, y_holdout, batch_size=BATCH_SIZE),\n",
    "        epochs=INITIAL_EPOCHS,\n",
    "        verbose=1)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== 评估与工具函数 ========================\n",
    "def evaluate_model(model: tf.keras.Model,\n",
    "                  X_test: np.ndarray,\n",
    "                  y_test: np.ndarray,\n",
    "                  round_num: int,\n",
    "                  save_dir: str) -> Tuple[float, float]:\n",
    "    \"\"\"模型评估与结果保存\"\"\"\n",
    "    # 预测\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # 保存报告\n",
    "    _save_classification_report(y_true, y_pred_classes, round_num, save_dir)\n",
    "    _save_confusion_matrix(y_true, y_pred_classes, round_num, save_dir)\n",
    "    \n",
    "    # 计算指标\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Round {round_num} - Test Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def _save_classification_report(y_true: np.ndarray,\n",
    "                              y_pred: np.ndarray,\n",
    "                              round_num: int,\n",
    "                              save_dir: str):\n",
    "    \"\"\"保存分类报告\"\"\"\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    os.makedirs(f\"{save_dir}/reports\", exist_ok=True)\n",
    "    with open(f\"{save_dir}/reports/classification_report_round_{round_num}.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "def _save_confusion_matrix(y_true: np.ndarray,\n",
    "                         y_pred: np.ndarray,\n",
    "                         round_num: int,\n",
    "                         save_dir: str):\n",
    "    \"\"\"保存混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix (Round {round_num})')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    os.makedirs(f\"{save_dir}/confusion_matrices\", exist_ok=True)\n",
    "    plt.savefig(f\"{save_dir}/confusion_matrices/round_{round_num}.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "def check_early_stopping(recent_accuracies: List[float],\n",
    "                        best_accuracy: float,\n",
    "                        no_improvement_count: int,\n",
    "                        window_size: int = WINDOW_SIZE,\n",
    "                        min_delta: float = MIN_DELTA,\n",
    "                        patience: int = PATIENCE) -> Tuple[bool, float, int]:\n",
    "    \"\"\"早停检查\"\"\"\n",
    "    current_accuracy = recent_accuracies[-1]\n",
    "    \n",
    "    # 更新最佳准确率\n",
    "    if current_accuracy > best_accuracy + min_delta:\n",
    "        best_accuracy = current_accuracy\n",
    "        no_improvement_count = 0\n",
    "        print(f\"New best accuracy: {best_accuracy:.4f}\")\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "    \n",
    "    # 滑动窗口检查\n",
    "    if len(recent_accuracies) >= window_size:\n",
    "        window_avg = np.mean(recent_accuracies[-window_size:])\n",
    "        prev_window_avg = np.mean(recent_accuracies[-window_size-1:-1] \n",
    "                                if len(recent_accuracies) > window_size \n",
    "                                else recent_accuracies[:-1])\n",
    "        \n",
    "        if window_avg <= prev_window_avg + min_delta:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"No improvement count: {no_improvement_count}/{patience}\")\n",
    "    \n",
    "    should_stop = no_improvement_count >= patience\n",
    "    return should_stop, best_accuracy, no_improvement_count\n",
    "\n",
    "def _save_final_results(model: tf.keras.Model,\n",
    "                      training_state: Dict[str, Any],\n",
    "                      output_dir: str):\n",
    "    \"\"\"保存最终结果和模型\"\"\"\n",
    "    # 保存训练结果\n",
    "    pd.DataFrame(training_state['results']).to_csv(\n",
    "        f\"{output_dir}/training_results.csv\", index=False)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    model.set_weights(training_state['best_model_weights'])\n",
    "    model.save(f\"{output_dir}/best_model.h5\")\n",
    "    \n",
    "    # 最终评估\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    _save_classification_report(y_true, y_pred_classes, 'final', output_dir)\n",
    "    _save_confusion_matrix(y_true, y_pred_classes, 'final', output_dir)\n",
    "    \n",
    "    model.save(f\"{output_dir}/final_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ======================== 全局配置 ========================\n",
    "# # 数据集参数\n",
    "# NUM_CLASSES = 4  # glioma, meningioma, notumor, pituitary\n",
    "# class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "\n",
    "# # 联邦学习参数\n",
    "# CLIENTS_PER_CLASS = 5  # 每类客户端数量\n",
    "# NUM_CLIENTS = NUM_CLASSES * CLIENTS_PER_CLASS  # 总客户端数\n",
    "# NUM_ROUNDS = 50  # 联邦学习总轮次\n",
    "# CLIENT_FRACTION = 0.5  # 每轮选择的客户端比例\n",
    "# HOLDOUT_RATIO = 0.2  # 保留数据比例\n",
    "# HOLDOUT_CLIENT_RATIO = 0.5  # 每个客户端获得的保留数据比例\n",
    "\n",
    "# # 训练参数\n",
    "# BATCH_SIZE = 16\n",
    "# CLIENT_EPOCHS = 5\n",
    "# INITIAL_EPOCHS = 10\n",
    "# LEARNING_RATE = 0.0001\n",
    "\n",
    "# # 早停参数\n",
    "# WINDOW_SIZE = 5\n",
    "# MIN_DELTA = 0.001\n",
    "# PATIENCE = 5\n",
    "\n",
    "# # ======================== 联邦学习核心函数 ========================\n",
    "# def select_clients(clients: List[Tuple[np.ndarray, np.ndarray]], \n",
    "#                   client_fraction: float = CLIENT_FRACTION) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "#     \"\"\"分层抽样选择客户端\"\"\"\n",
    "#     selected_indices = []\n",
    "#     for class_id in range(NUM_CLASSES):\n",
    "#         class_client_indices = range(class_id*CLIENTS_PER_CLASS, (class_id+1)*CLIENTS_PER_CLASS)\n",
    "#         selected = np.random.choice(\n",
    "#             list(class_client_indices),\n",
    "#             size=max(1, int(client_fraction * CLIENTS_PER_CLASS)),\n",
    "#             replace=False)\n",
    "#         selected_indices.extend(selected)\n",
    "#     return [clients[i] for i in selected_indices]\n",
    "\n",
    "# def client_train(global_model: tf.keras.Model,\n",
    "#                 client_data: Tuple[np.ndarray, np.ndarray],\n",
    "#                 epochs: int = CLIENT_EPOCHS) -> Tuple[List[np.ndarray], int]:\n",
    "#     \"\"\"客户端本地训练\"\"\"\n",
    "#     # 克隆全局模型\n",
    "#     client_model = clone_model(global_model)\n",
    "#     client_model.set_weights(global_model.get_weights())\n",
    "    \n",
    "#     # 编译模型\n",
    "#     client_model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "#                        loss='categorical_crossentropy',\n",
    "#                        metrics=['accuracy'])\n",
    "    \n",
    "#     # 训练\n",
    "#     client_model.fit(\n",
    "#         datagen.flow(client_data[0], client_data[1], batch_size=BATCH_SIZE),\n",
    "#         epochs=epochs,\n",
    "#         verbose=1)\n",
    "    \n",
    "#     return client_model.get_weights(), len(client_data[0])\n",
    "\n",
    "# def federated_average(global_model: tf.keras.Model,\n",
    "#                      client_weights: List[List[np.ndarray]],\n",
    "#                      client_sizes: List[int]) -> List[np.ndarray]:\n",
    "#     \"\"\"联邦平均聚合\"\"\"\n",
    "#     global_weights = []\n",
    "#     total_size = sum(client_sizes)\n",
    "    \n",
    "#     for layer_idx in range(len(global_model.get_weights())):\n",
    "#         weighted_sum = np.zeros_like(global_model.get_weights()[layer_idx])\n",
    "#         for client_idx in range(len(client_weights)):\n",
    "#             weighted_sum += client_weights[client_idx][layer_idx] * (client_sizes[client_idx] / total_size)\n",
    "#         global_weights.append(weighted_sum)\n",
    "    \n",
    "#     return global_weights\n",
    "\n",
    "\n",
    "\n",
    "# # ======================== 主流程 ========================\n",
    "# def run_federated_learning(X_train: np.ndarray,\n",
    "#                          y_train: np.ndarray,\n",
    "#                          X_test: np.ndarray,\n",
    "#                          y_test: np.ndarray,\n",
    "#                          output_dir: str = \"results\") -> Dict[str, Any]:\n",
    "#     \"\"\"执行完整的联邦学习流程\"\"\"\n",
    "#     # 准备目录\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # 准备数据\n",
    "#     clients, X_holdout, y_holdout = prepare_data(X_train, y_train)\n",
    "    \n",
    "#     # 初始化全局模型\n",
    "#     global_model = initialize_global_model(X_holdout, y_holdout)\n",
    "    \n",
    "#     # 初始评估\n",
    "#     initial_loss, initial_accuracy = evaluate_model(\n",
    "#         global_model, X_test, y_test, 0, output_dir)\n",
    "    \n",
    "#     # 初始化训练状态\n",
    "#     training_state = {\n",
    "#         'results': [{'round': 0, 'accuracy': initial_accuracy, 'loss': initial_loss}],\n",
    "#         'best_accuracy': initial_accuracy,\n",
    "#         'best_model_weights': global_model.get_weights(),\n",
    "#         'recent_accuracies': [initial_accuracy],\n",
    "#         'no_improvement_count': 0,\n",
    "#         'early_stop_triggered': False\n",
    "#     }\n",
    "\n",
    "#     # 联邦学习主循环\n",
    "#     for round_num in range(1, NUM_ROUNDS + 1):\n",
    "#         if training_state['early_stop_triggered']:\n",
    "#             print(\"Early stopping triggered. Ending training.\")\n",
    "#             break\n",
    "            \n",
    "#         print(f\"\\n=== Federated Learning Round {round_num}/{NUM_ROUNDS} ===\")\n",
    "        \n",
    "#         # 1. 客户端选择\n",
    "#         selected_clients = select_clients(clients)\n",
    "        \n",
    "#         # 2. 本地训练\n",
    "#         client_weights, client_sizes = [], []\n",
    "#         for client_data in selected_clients:\n",
    "#             weights, size = client_train(global_model, client_data)\n",
    "#             client_weights.append(weights)\n",
    "#             client_sizes.append(size)\n",
    "        \n",
    "#         # 3. 模型聚合\n",
    "#         global_weights = federated_average(global_model, client_weights, client_sizes)\n",
    "#         global_model.set_weights(global_weights)\n",
    "        \n",
    "#         # 4. 评估\n",
    "#         loss, accuracy = evaluate_model(\n",
    "#             global_model, X_test, y_test, round_num, output_dir)\n",
    "        \n",
    "#         # 更新训练状态\n",
    "#         training_state['results'].append({\n",
    "#             'round': round_num,\n",
    "#             'accuracy': accuracy,\n",
    "#             'loss': loss\n",
    "#         })\n",
    "#         training_state['recent_accuracies'].append(accuracy)\n",
    "        \n",
    "#         # 5. 早停检查\n",
    "#         should_stop, new_best, new_count = check_early_stopping(\n",
    "#             training_state['recent_accuracies'],\n",
    "#             training_state['best_accuracy'],\n",
    "#             training_state['no_improvement_count'])\n",
    "            \n",
    "#         training_state.update({\n",
    "#             'best_accuracy': new_best,\n",
    "#             'no_improvement_count': new_count,\n",
    "#             'early_stop_triggered': should_stop\n",
    "#         })\n",
    "        \n",
    "#         # 保存最佳模型\n",
    "#         if new_best == accuracy:\n",
    "#             training_state['best_model_weights'] = global_model.get_weights()\n",
    "\n",
    "#     # 保存最终结果\n",
    "#     _save_final_results(global_model, training_state, output_dir)\n",
    "    \n",
    "#     return training_state\n",
    "\n",
    "# # ======================== 数据增强 ========================\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rotation_range=10,\n",
    "#     width_shift_range=0.05,\n",
    "#     height_shift_range=0.05,\n",
    "#     horizontal_flip=True)\n",
    "\n",
    "# # ======================== 执行入口 ========================\n",
    "# if __name__ == \"__main__\":\n",
    "#     # X_train, y_train, X_test, y_test = load_your_dataset()\n",
    "    \n",
    "#     # 执行联邦学习\n",
    "#     results = run_federated_learning(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "#     print(\"\\nTraining completed. Results saved to 'results' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion Matrix Visualization\n",
    "# from sklearn.metrics import classification_report,accuracy_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "# # Prepare data\n",
    "# if len(y_test.shape) > 1 and y_test.shape[1] > 1:  # If one-hot encoded\n",
    "#     y_test_labels = np.argmax(y_test, axis=1)\n",
    "# else:\n",
    "#     y_test_labels = y_test\n",
    "    \n",
    "# # Predict on the test set (using a small batch_size to prevent OOM)\n",
    "# y_pred = global_model.predict(X_test, batch_size=8)\n",
    "# y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# # Define class names (adjust according to your dataset)\n",
    "# class_names = ['glioma', 'meningioma', 'no_tumor', 'pituitary']\n",
    "\n",
    "# # Print the full classification report\n",
    "# print(\"=\"*65)\n",
    "# print(\"Test Set Classification Report:\")\n",
    "# print(\"=\"*65)\n",
    "# print(classification_report(y_test_labels, y_pred_labels, \n",
    "#                            target_names=class_names, digits=4))\n",
    "\n",
    "# # Print overall accuracy\n",
    "# test_loss, test_acc = global_model.evaluate(X_test, y_test, \n",
    "#                                            batch_size=8, verbose=0)\n",
    "# print(f\"\\nOverall Test Set Accuracy: {test_acc:.4f}, Loss: {test_loss:.4f}\")\n",
    "# print(\"=\"*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import os\n",
    "\n",
    "# # 确保results目录存在\n",
    "# os.makedirs('results621', exist_ok=True)\n",
    "\n",
    "# # 1. 加载模型\n",
    "\n",
    "# print(\"Loading model...\")\n",
    "# model = load_model('results621/best_global_model.h5')\n",
    "# print(\"Model loaded successfully.\\n\")\n",
    "\n",
    "# # 重新编译模型（使用原始训练时的相同参数）\n",
    "# model.compile(optimizer=Adam(learning_rate=0.001),  # 使用与原训练相同的优化器\n",
    "#               loss='categorical_crossentropy',     # 分类任务常用loss\n",
    "#               metrics=['accuracy'])               # 监控准确率\n",
    "\n",
    "\n",
    "# # 2. 准备数据\n",
    "# if len(y_test.shape) > 1 and y_test.shape[1] > 1:  # 如果是one-hot编码\n",
    "#     y_test_labels = np.argmax(y_test, axis=1)\n",
    "# else:\n",
    "#     y_test_labels = y_test\n",
    "\n",
    "# # 3. 预测测试集（使用小batch_size防止内存不足）\n",
    "# print(\"Making predictions...\")\n",
    "# y_pred = model.predict(X_test, batch_size=8)\n",
    "# y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# # 4. 定义类别名称（根据你的数据集调整）\n",
    "# class_names = ['glioma', 'meningioma', 'no_tumor', 'pituitary']\n",
    "\n",
    "# # 5. 计算并打印分类报告\n",
    "# print(\"=\"*65)\n",
    "# print(\"Generating classification report...\")\n",
    "# report = classification_report(y_test_labels, y_pred_labels, \n",
    "#                               target_names=class_names, digits=4)\n",
    "\n",
    "# # 6. 计算整体准确率\n",
    "# print(\"Evaluating model...\")\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=8, verbose=0)\n",
    "\n",
    "# # 7. 打印结果\n",
    "# print(\"=\"*65)\n",
    "# print(\"Test Set Classification Report:\")\n",
    "# print(\"=\"*65)\n",
    "# print(report)\n",
    "# print(f\"\\nOverall Test Set Accuracy: {test_acc:.4f}, Loss: {test_loss:.4f}\")\n",
    "# print(\"=\"*65)\n",
    "\n",
    "# # 8. 将结果保存到文件\n",
    "# report_path = 'results621/classification_report.txt'\n",
    "# with open(report_path, 'w') as f:\n",
    "#     f.write(\"=\"*65 + \"\\n\")\n",
    "#     f.write(\"Test Set Classification Report:\\n\")\n",
    "#     f.write(\"=\"*65 + \"\\n\")\n",
    "#     f.write(report)\n",
    "#     f.write(f\"\\nOverall Test Set Accuracy: {test_acc:.4f}, Loss: {test_loss:.4f}\\n\")\n",
    "#     f.write(\"=\"*65 + \"\\n\")\n",
    "\n",
    "# print(f\"\\nClassification report saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Confusion Matrix Visualization\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "# # Prepare data\n",
    "# if len(y_test.shape) > 1 and y_test.shape[1] > 1:  # If one-hot encoded\n",
    "#     y_test_labels = np.argmax(y_test, axis=1)\n",
    "# else:\n",
    "#     y_test_labels = y_test\n",
    "\n",
    "# y_pred = global_model.predict(X_test, batch_size=8)\n",
    "# y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# # Define class names (consistent with classification report)\n",
    "# class_names = ['glioma', 'meningioma', 'no_tumor', 'pituitary']\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# # Plot polished confusion matrix\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.set(font_scale=1.2)\n",
    "# ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "#                  cbar=False, square=True,\n",
    "#                  xticklabels=class_names, \n",
    "#                  yticklabels=class_names)\n",
    "\n",
    "# # Enhance visualization\n",
    "# ax.set(xlabel='Predicted Label', ylabel='True Label', \n",
    "#        title='Confusion Matrix')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Add colorbar on the right\n",
    "# plt.colorbar(ax.collections[0], pad=0.15, label=\"Sample Count\")\n",
    "\n",
    "# # Save high-quality image (optional)\n",
    "# plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import os\n",
    "\n",
    "# # 确保results目录存在\n",
    "# os.makedirs('results621', exist_ok=True)\n",
    "\n",
    "# # 1. 加载模型\n",
    "# print(\"Loading model...\")\n",
    "# model = load_model('results621/best_global_model.h5')\n",
    "\n",
    "# # 2. 重新编译模型\n",
    "# print(\"Compiling model...\")\n",
    "# model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "#               loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # 3. 准备数据\n",
    "# if len(y_test.shape) > 1 and y_test.shape[1] > 1:  # 如果是one-hot编码\n",
    "#     y_test_labels = np.argmax(y_test, axis=1)\n",
    "# else:\n",
    "#     y_test_labels = y_test\n",
    "\n",
    "# # 4. 预测测试集\n",
    "# print(\"Making predictions...\")\n",
    "# y_pred = model.predict(X_test, batch_size=8)\n",
    "# y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# # 5. 定义类别名称\n",
    "# class_names = ['glioma', 'meningioma', 'no_tumor', 'pituitary']\n",
    "\n",
    "# # 6. 计算混淆矩阵\n",
    "# print(\"Computing confusion matrix...\")\n",
    "# cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# # 7. 绘制混淆矩阵\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.set(font_scale=1.2, style='whitegrid')  # 设置样式\n",
    "# ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "#                 cbar=False, square=True,\n",
    "#                 xticklabels=class_names,\n",
    "#                 yticklabels=class_names)\n",
    "\n",
    "# # 8. 美化图表\n",
    "# ax.set(xlabel='Predicted Label', ylabel='True Label',\n",
    "#       title='Confusion Matrix')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # 9. 添加颜色条\n",
    "# plt.colorbar(ax.collections[0], pad=0.15, label=\"Sample Count\")\n",
    "\n",
    "# # 10. 保存为PDF\n",
    "# output_path = 'results621/confusion_matrix-15clients.pdf'\n",
    "# plt.savefig(output_path,\n",
    "#            format='pdf',\n",
    "#            dpi=300,\n",
    "#            bbox_inches='tight',\n",
    "#            pad_inches=0.1,\n",
    "#            transparent=False)\n",
    "\n",
    "# print(f\"\\nConfusion matrix saved to {output_path}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications.vgg19 import VGG19\n",
    "# IMG_SIZE=(200,200)\n",
    "# base_model = VGG19(\n",
    "#     include_top=False,\n",
    "#     input_shape=IMG_SIZE + (3,),\n",
    "#     weights='imagenet')\n",
    "\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Customized layers\n",
    "# x = GlobalAveragePooling2D()(base_model.output)\n",
    "# x = Dropout(0.4)(x)\n",
    "# predict = Dense(4,activation='softmax')(x)\n",
    "\n",
    "# # create a model object\n",
    "# model = Model(inputs = base_model.input,outputs = predict)\n",
    "# # model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #compile our model.\n",
    "# adam = Adam(learning_rate=0.0001)\n",
    "# model.compile(optimizer=adam, loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer Definition:\n",
    "An instance of the Adam optimizer is defined here. Adam is a widely-used optimization algorithm in deep learning, known for its efficiency and computational effectiveness. It combines the benefits of both momentum and RMSProp, and adaptively adjusts the learning rate during training. By setting learning_rate=0.0001, a relatively small learning rate is specified, which can help the model converge more stably, albeit potentially requiring more epochs to reach optimal performance.\n",
    "\n",
    "Model Compilation:\n",
    "optimizer=adam: Specifies that the previously defined Adam optimizer should be used to perform backpropagation and weight updates during training.\n",
    "loss='categorical_crossentropy': Chooses \"categorical crossentropy\" as the loss function. This is a commonly used loss function for multi-class classification problems, particularly when the output layer uses a softmax activation. Categorical crossentropy measures the difference between two probability distributions – in this case, the true distribution of labels versus the predicted probabilities by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_len = len(X_train)\n",
    "# val_len = len(X_val)\n",
    "# print(\"-----------Training Data length-----------------\")\n",
    "# print(train_len)\n",
    "\n",
    "# print(\"-----------Validation Data length-----------------\")\n",
    "# print(val_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = model.fit(datagen.flow(X_train, y_train, batch_size=32),validation_data = (X_val,y_val),epochs =25,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plotting the results\n",
    "# acc = hist.history['accuracy']\n",
    "# val_acc = hist.history['val_accuracy']\n",
    "# loss = hist.history['loss']\n",
    "# val_loss = hist.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,3))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.plot(acc,label='Training Accuracy')\n",
    "# plt.plot(val_acc,label='Validation Accuracy')\n",
    "# plt.legend(loc= \"lower right\")\n",
    "# plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(loss,label='Training Loss')\n",
    "# plt.plot(val_loss,label='Validation Loss')\n",
    "# plt.legend(loc= \"upper right\")\n",
    "# plt.title(\"Training and Validation Loss\")\n",
    "# plt.savefig('VGGAccLoss.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Saving the model\n",
    "# model.save('bestvgg19-2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 2s 39ms/step - loss: 0.3032 - accuracy: 0.9367\n"
     ]
    }
   ],
   "source": [
    "loss,acc = global_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report,accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# predicted_classes = np.argmax(global_model.predict(X_test), axis = 1)\n",
    "# print(classification_report(np.argmax(y_test,axis=1), predicted_classes,target_names=['glioma','meningioma','no_tumor','pituitary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# # pred_Y = global_model.predict(X_test, batch_size = 8, verbose = True)\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "#     \"\"\"\n",
    "#     This function prints and plots the confusion matrix.\n",
    "#     Normalization can be applied by setting `normalize=True`.\n",
    "#     \"\"\"\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     target_names=['glioma','meningioma','no_tumor','pituitary']\n",
    "\n",
    "#     if target_names is not None:\n",
    "#         tick_marks = np.arange(len(target_names))\n",
    "#         plt.xticks(tick_marks, target_names, rotation=45)\n",
    "#         plt.yticks(tick_marks, target_names)\n",
    "    \n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, cm[i, j],\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "\n",
    "# # Predict the values from the validation dataset\n",
    "# Y_pred = global_model.predict(X_test, batch_size=8)\n",
    "# # Convert predictions classes to one hot vectors \n",
    "# Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# # Convert validation observations to one hot vectors\n",
    "# # compute the confusion matrix\n",
    "# rounded_labels=np.argmax(y_test, axis=1)\n",
    "# confusion_mtx = confusion_matrix(rounded_labels, Y_pred_classes)\n",
    "\n",
    " \n",
    "\n",
    "# # plot the confusion matrix\n",
    "# plot_confusion_matrix(confusion_mtx, classes = range(4)) \n",
    "# plt.savefig('VGGCM.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用Seaborn的改进版本（无十字线+更美观）\n",
    "# import seaborn as sns\n",
    "\n",
    "# # 预测\n",
    "# y_pred = global_model.predict(X_test, batch_size=8)\n",
    "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# # 混淆矩阵\n",
    "# cm = confusion_matrix(y_true, y_pred_classes)\n",
    "# class_names = ['glioma', 'meningioma', 'no_tumor', 'pituitary']\n",
    "\n",
    "# # 绘图\n",
    "# plt.figure(figsize=(10,8))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "#             xticklabels=class_names,\n",
    "#             yticklabels=class_names,\n",
    "#             cbar=False,\n",
    "#             linewidths=0.5,\n",
    "#             linecolor='lightgray')\n",
    "# plt.title('Confusion Matrix (No Tumor)', pad=20)\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.xlabel('Predicted Label', labelpad=10)\n",
    "# plt.ylabel('True Label', labelpad=10)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('Confusion_Matrix.pdf', bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat = model.predict(X_test)\n",
    "\n",
    "# # define text labels \n",
    "# target_labels = ['glioma','meningioma','no_tumor','pituitary']\n",
    "\n",
    "# # plot a random sample of test images, their predicted labels, and ground truth\n",
    "# fig = plt.figure(figsize=(20, 8))\n",
    "# for i, idx in enumerate(np.random.choice(X_test.shape[0], size=12, replace=False)):\n",
    "#     ax = fig.add_subplot(4,4, i+1, xticks=[], yticks=[])\n",
    "#     ax.imshow(np.squeeze(X_test[idx]))\n",
    "#     pred_idx = np.argmax(y_hat[idx])\n",
    "#     true_idx = np.argmax(y_test[idx])\n",
    "#     ax.set_title(\"{} ({})\".format(target_labels[pred_idx], target_labels[true_idx]),\n",
    "#                  color=(\"blue\" if pred_idx == true_idx else \"orange\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('bestvgg19-2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import pandas as pd\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "# #from scipy import interp\n",
    "# from itertools import cycle\n",
    "# import pandas as pd\n",
    "# %matplotlib inline\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = np.array(y_test)\n",
    "\n",
    "# n_classes = 4\n",
    "\n",
    "# pred_Y = model.predict(X_test, batch_size = 16, verbose = True)\n",
    "# # Plot linewidth.\n",
    "# lw = 2\n",
    "\n",
    "# # Compute ROC curve and ROC area for each class\n",
    "\n",
    "\n",
    "# # Compute ROC curve and ROC area for each class\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# for i in range(n_classes):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred_Y[:, i])\n",
    "#     roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "#     # Compute micro-average ROC curve and ROC area\n",
    "# fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), pred_Y.ravel())\n",
    "# roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "# # Plot of a ROC curve for a specific class\n",
    "# for i in range(n_classes):\n",
    "#     plt.figure()\n",
    "#     plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "#     plt.plot([0, 1], [0, 1], 'k--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('Receiver operating characteristic example')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.show()\n",
    "\n",
    "# # First aggregate all false positive rates\n",
    "# all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# # Then interpolate all ROC curves at this points\n",
    "# mean_tpr = np.zeros_like(all_fpr)\n",
    "# for i in range(n_classes):\n",
    "#     mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# # Finally average it and compute AUC\n",
    "# mean_tpr /= n_classes\n",
    "\n",
    "# fpr[\"macro\"] = all_fpr\n",
    "# tpr[\"macro\"] = mean_tpr\n",
    "# roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# # Plot all ROC curves\n",
    "# fig = plt.figure(figsize=(12, 8))\n",
    "# plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "#          label='micro-average ROC curve (area = {0:0.2f})'\n",
    "#                ''.format(roc_auc[\"micro\"]),\n",
    "#          color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "# plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "#          label='macro-average ROC curve (area = {0:0.2f})'\n",
    "#                ''.format(roc_auc[\"macro\"]),\n",
    "#          color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "# colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "# for i, color in zip(range(n_classes), colors):\n",
    "#     plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "#              label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "#              ''.format(i, roc_auc[i]))\n",
    "\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# sns.despine()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "# model = load_model('bestvgg19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the labels\n",
    "# labels = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
    "\n",
    "# # Define the directories\n",
    "# directories = {\n",
    "#     'glioma': './Dataset/Testingprediction/glioma',\n",
    "#     'meningioma': './Dataset/Testingprediction/meningioma',\n",
    "#     'notumor': './Dataset/Testingprediction/notumor',\n",
    "#     'pituitary': './Dataset/Testingprediction/pituitary'\n",
    "# }\n",
    "\n",
    "# # Function to measure prediction time for images in a directory\n",
    "# def measure_prediction_time(model, directory):\n",
    "#     times = []\n",
    "#     for img_name in os.listdir(directory):\n",
    "#         img_path = os.path.join(directory, img_name)\n",
    "#         img = image.load_img(img_path, target_size=(200, 200))\n",
    "#         x = image.img_to_array(img)\n",
    "#         x = x / 255.0\n",
    "#         x = np.expand_dims(x, axis=0)\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         predict_image = model.predict(x)\n",
    "#         end_time = time.time()\n",
    "        \n",
    "#         prediction_time = end_time - start_time\n",
    "#         times.append(prediction_time)\n",
    "    \n",
    "#     return times\n",
    "\n",
    "# # Dictionary to store times for each category\n",
    "# prediction_times = {}\n",
    "\n",
    "# # Measure prediction times for each category\n",
    "# for label, directory in directories.items():\n",
    "#     times = measure_prediction_time(model, directory)\n",
    "#     prediction_times[label] = times\n",
    "#     avg_time = np.mean(times)\n",
    "#     max_time = np.max(times)\n",
    "#     min_time = np.min(times)\n",
    "#     print(f\"{label}: Average time = {avg_time:.4f} seconds, Max time = {max_time:.4f} seconds, Min time = {min_time:.4f} seconds\")\n",
    "\n",
    "# # If you want to print the overall results for all categories\n",
    "# overall_times = [time for times in prediction_times.values() for time in times]\n",
    "# avg_time = np.mean(overall_times)\n",
    "# max_time = np.max(overall_times)\n",
    "# min_time = np.min(overall_times)\n",
    "# print(f\"Overall: Average time = {avg_time:.4f} seconds, Max time = {max_time:.4f} seconds, Min time = {min_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# # Define the labels\n",
    "# labels = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
    "\n",
    "# # Define the directories\n",
    "# directories = {\n",
    "#     'glioma': './Dataset/Testingprediction/glioma',\n",
    "#     'meningioma': './Dataset/Testingprediction/meningioma',\n",
    "#     'notumor': './Dataset/Testingprediction/notumor',\n",
    "#     'pituitary': './Dataset/Testingprediction/pituitary'\n",
    "# }\n",
    "\n",
    "# # Function to measure prediction time for images in a directory\n",
    "# def measure_prediction_time(model, directory):\n",
    "#     times = []\n",
    "#     for img_name in os.listdir(directory):\n",
    "#         img_path = os.path.join(directory, img_name)\n",
    "#         img = image.load_img(img_path, target_size=(200, 200))\n",
    "#         x = image.img_to_array(img)\n",
    "#         x = x / 255.0\n",
    "#         x = np.expand_dims(x, axis=0)\n",
    "        \n",
    "#         start_time = time.time()\n",
    "#         predict_image = model.predict(x)\n",
    "#         end_time = time.time()\n",
    "        \n",
    "#         prediction_time = end_time - start_time\n",
    "#         times.append(prediction_time)\n",
    "    \n",
    "#     return times\n",
    "\n",
    "# # Dictionary to store times for each category\n",
    "# prediction_times = {}\n",
    "\n",
    "# # Measure prediction times for each category\n",
    "# for label, directory in directories.items():\n",
    "#     times = measure_prediction_time(model, directory)\n",
    "#     prediction_times[label] = times\n",
    "\n",
    "# # Create a DataFrame to organize the results\n",
    "# results = {\n",
    "#     'Category': [],\n",
    "#     'Average Time (s)': [],\n",
    "#     'Max Time (s)': [],\n",
    "#     'Min Time (s)': []\n",
    "# }\n",
    "\n",
    "# for label, times in prediction_times.items():\n",
    "#     avg_time = np.mean(times)\n",
    "#     max_time = np.max(times)\n",
    "#     min_time = np.min(times)\n",
    "#     results['Category'].append(label)\n",
    "#     results['Average Time (s)'].append(avg_time)\n",
    "#     results['Max Time (s)'].append(max_time)\n",
    "#     results['Min Time (s)'].append(min_time)\n",
    "\n",
    "# # Add overall statistics\n",
    "# overall_times = [time for times in prediction_times.values() for time in times]\n",
    "# avg_time = np.mean(overall_times)\n",
    "# max_time = np.max(overall_times)\n",
    "# min_time = np.min(overall_times)\n",
    "# results['Category'].append('Overall')\n",
    "# results['Average Time (s)'].append(avg_time)\n",
    "# results['Max Time (s)'].append(max_time)\n",
    "# results['Min Time (s)'].append(min_time)\n",
    "\n",
    "# # Create a DataFrame and print it\n",
    "# df = pd.DataFrame(results)\n",
    "# print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
